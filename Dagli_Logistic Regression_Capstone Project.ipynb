{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/cesurdagli/Desktop/Capstone Project/Feb17NoNull_Merged.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pass_fail1']=df['pass_fail'].map({'PASS':1, 'FAIL':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing empty values with null values\n",
    "df=df.replace(r'^\\s+$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dropping the null values\n",
    "df=df[df['reason'].notnull() & df['highestEd'].notnull() & df['level'].notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>highestEd</th>\n",
       "      <th>reason</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   highestEd  reason  level\n",
       "0          2       2      2\n",
       "1          3       2      2\n",
       "2          3       2      2\n",
       "3          4       2      1\n",
       "4          2       2      2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recoding the categorical values\n",
    "clean_nums = {'highestEd': {'DD':5, 'MD':4, 'BD':3, 'HS':2, 'AD':1, 'OT':0},\n",
    "                  'reason': {'review':4, 'learn':3, 'assignment':2, 'curious':1, 'other':0},\n",
    "                  'level': {'confident':2, 'some':1, 'little':0},\n",
    "             'testLevel': {'GR':1, 'UG':0}}\n",
    "df.replace(clean_nums, inplace=True)\n",
    "df[['highestEd', 'reason', 'level']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'email', 'pass_fail', 'right_question', 'wrong_question',\n",
       "       'testID', 'testTakenTime', 'testDuration', 'testLevel', 'taskCompleted',\n",
       "       'viewedVideoCases', 'viewedDemVideos', 'integrationCompleted',\n",
       "       'practiceTestTaken', 'quality_Satisfaction1', 'authenticProblems1',\n",
       "       'demonstration1', 'application1', 'activation1', 'ALT2', 'ALT3',\n",
       "       'quality_Satisfaction2', 'demonstration2', 'demonstration3', 'ALT4',\n",
       "       'authenticProblems2', 'integration1', 'activation2',\n",
       "       'authenticProblems3', 'integration2', 'application3', 'integration3',\n",
       "       'activation3', 'application4', 'demonstration5', 'comments',\n",
       "       'ALT1Reversed', 'demonstration4Reversed',\n",
       "       'quality_Satisfaction3Reversed', 'quality_Satisfaction4Reversed', 'age',\n",
       "       'highestEd', 'degreeOther', 'reason', 'reasonOther', 'level',\n",
       "       'ave_quality_Satisfaction', 'ave_authenticProblems', 'ave_ALT',\n",
       "       'ave_activation', 'ave_demonstration', 'ave_application',\n",
       "       'ave_integration', 'pass_fail1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "The problem I would like to solve is to predict whether not a student will pass a certification test based on the data available.\n",
    "\n",
    "Various algorithms will be used to address this binary classification problem.\n",
    "\n",
    "I will use Logistic Regression, which is a statistical method for predicting binary classes, for this classification problem to predict student mastery level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  <class 'numpy.ndarray'> (9579, 29)\n",
      "y:  <class 'numpy.ndarray'> (9579,)\n",
      "df.pass_fail shape:  (9579,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Define the features\n",
    "X=df[['testLevel', 'highestEd', 'reason','level', 'quality_Satisfaction1', 'authenticProblems1',\n",
    "       'demonstration1', 'application1', 'activation1', 'ALT2', 'ALT3',\n",
    "       'quality_Satisfaction2', 'demonstration2', 'demonstration3', 'ALT4',\n",
    "       'authenticProblems2', 'integration1', 'activation2',\n",
    "       'authenticProblems3', 'integration2', 'application3', 'integration3',\n",
    "       'activation3', 'application4', 'demonstration5',\n",
    "       'ALT1Reversed', 'demonstration4Reversed',\n",
    "       'quality_Satisfaction3Reversed', 'quality_Satisfaction4Reversed']].values\n",
    "print(\"X: \", type(X), X.shape)\n",
    "\n",
    "# Define the target\n",
    "y = (df.pass_fail1 == 1).values\n",
    "print(\"y: \", type(y), y.shape)\n",
    "print(\"df.pass_fail shape: \", df.pass_fail1.shape)\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "Xlr, Xtestlr, ylr, ytestlr = train_test_split(X, y, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Xlr: [[1. 3. 2. ... 5. 5. 5.]\n",
      " [0. 1. 2. ... 5. 5. 5.]\n",
      " [1. 3. 2. ... 4. 5. 5.]\n",
      " ...\n",
      " [0. 0. 2. ... 3. 3. 3.]\n",
      " [0. 2. 2. ... 4. 4. 4.]\n",
      " [0. 2. 2. ... 3. 3. 3.]] <class 'numpy.ndarray'> (7184, 29) 7184\n",
      "\n",
      "\n",
      "Xtestlr [[1. 3. 2. ... 4. 4. 4.]\n",
      " [0. 3. 2. ... 3. 4. 4.]\n",
      " [0. 2. 4. ... 3. 5. 3.]\n",
      " ...\n",
      " [0. 2. 2. ... 4. 4. 4.]\n",
      " [1. 3. 2. ... 3. 4. 4.]\n",
      " [0. 2. 2. ... 3. 3. 3.]] <class 'numpy.ndarray'> (2395, 29) 2395\n",
      "\n",
      "\n",
      "ylr [ True False  True ... False False False] <class 'numpy.ndarray'> (7184,) 7184\n",
      "\n",
      "\n",
      "ytestlr [ True False False ...  True False False] <class 'numpy.ndarray'> (2395,) 2395\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\")\n",
    "print(\"Xlr:\", Xlr, type(Xlr), Xlr.shape, len(Xlr)) #TrainX\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Xtestlr\", Xtestlr, type(Xtestlr), Xtestlr.shape, len(Xtestlr)) #TestX\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"ylr\", ylr, type(ylr), ylr.shape, len(ylr)) #Trainy\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"ytestlr\", ytestlr, type(ytestlr), ytestlr.shape, len(ytestlr)) #Testy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Test] Accuracy score (y_predict_test, ytestlr): 0.7803757828810021\n",
      "\n",
      "\n",
      "[Training] Accuracy score: (ylr, y_predict_training) 0.7700445434298441\n"
     ]
    }
   ],
   "source": [
    "# Construct the LogisticRegression model\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data.\n",
    "clf.fit(Xlr, ylr) \n",
    "\n",
    "# Print the accuracy from the testing data.\n",
    "\n",
    "y_predict_test = clf.predict(Xtestlr)\n",
    "print(\"\\n\")\n",
    "print(\"[Test] Accuracy score (y_predict_test, ytestlr):\",accuracy_score(y_predict_test, ytestlr))\n",
    "\n",
    "\n",
    "# Printout the training score\n",
    "y_predict_training = clf.predict(Xlr)\n",
    "print(\"\\n\")\n",
    "print(\"[Training] Accuracy score: (ylr, y_predict_training)\",accuracy_score(ylr, y_predict_training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model's test accuracy (0.78) is pretty close to the training accuracy (0.77), then I can say that there is no \"variance\" between the training accuracy and the test accuracy. This is an indication that the model will \"generalize well\", which means that the model will be well-behaved when new data is presented to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1852,   16],\n",
       "       [ 510,   17]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(ytestlr, y_predict_test)\n",
    "cnf_matrix\n",
    "\n",
    "# In the output, 1869 (1852+17) are actual predictions, and 526 (510+16) are incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7803757828810021\n",
      "Precision: 0.5151515151515151\n",
      "Recall: 0.027239709443099273\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_predict_test, ytestlr))\n",
    "print(\"Precision:\",metrics.precision_score(ytestlr, y_predict_test))\n",
    "print(\"Recall:\",metrics.recall_score(ylr, y_predict_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try multiple splits and see the effects, with the same proportion 75% train and 25% test\n",
    "\n",
    "# keep lists for training accuracy, test accuracy, and the difference\n",
    "# between the latter and the former\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "accuracy_difference = []\n",
    "\n",
    "# define number of trials\n",
    "n_splits = 1000\n",
    "\n",
    "# define LogisticRegression object\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# loop over multiple trials\n",
    "for i in range(0, n_splits):\n",
    "    # do split for this iteration, using shuffle and a different random_state \n",
    "    # for every iteration\n",
    "    Xlr, Xtestlr, ylr, ytestlr = train_test_split(df[['testLevel', 'highestEd', 'reason','level', 'quality_Satisfaction1', 'authenticProblems1',\n",
    "       'demonstration1', 'application1', 'activation1', 'ALT2', 'ALT3',\n",
    "       'quality_Satisfaction2', 'demonstration2', 'demonstration3', 'ALT4',\n",
    "       'authenticProblems2', 'integration1', 'activation2',\n",
    "       'authenticProblems3', 'integration2', 'application3', 'integration3',\n",
    "       'activation3', 'application4', 'demonstration5',\n",
    "       'ALT1Reversed', 'demonstration4Reversed',\n",
    "       'quality_Satisfaction3Reversed', 'quality_Satisfaction4Reversed']].values, \n",
    "                                              (df.pass_fail1 == 1).values,random_state=i)\n",
    "    # fit on the training set\n",
    "    clf.fit(Xlr, ylr)\n",
    "    \n",
    "    # predict on training set\n",
    "    y_predict_training = clf.predict(Xlr)\n",
    "    \n",
    "    # predict on test set\n",
    "    y_predict_test = clf.predict(Xtestlr)\n",
    "    \n",
    "    # save training accuracy for this split\n",
    "    tr_accuracy = accuracy_score(y_predict_training, ylr)\n",
    "    training_accuracy.append(tr_accuracy)\n",
    "    \n",
    "    # save test accuracy for this split\n",
    "    tst_accuracy = accuracy_score(y_predict_test, ytestlr)\n",
    "    test_accuracy.append(tst_accuracy)\n",
    "    \n",
    "    # save difference\n",
    "    accuracy_difference.append(tst_accuracy - tr_accuracy)\n",
    "#end for\n",
    "#\n",
    "# plot both curves\n",
    "# ref: https://matplotlib.org/users/pyplot_tutorial.html\n",
    "# parameter alpha below is a percentage of transparency\n",
    "# ref: https://matplotlib.org/users/legend_guide.html\n",
    "training_plot, = plt.plot(training_accuracy, 'b')\n",
    "test_plot, = plt.plot(test_accuracy, 'r', alpha = 0.35)\n",
    "plt.legend([training_plot, test_plot], ['Training Accuracy', 'Test Accuracy'])\n",
    "plt.ylabel('Training vs Test Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Accuracy difference plot\n",
    "diff_plot, = plt.plot(accuracy_difference, 'y')\n",
    "plt.legend([diff_plot], ['Difference'])\n",
    "plt.ylabel('Test/Training Difference Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn.metrics.classification_report for a more comprehensive\n",
    "# performance analysis\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "# ref: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n",
    "\n",
    "print(\"[Training Classification Report:]\")\n",
    "print(classification_report(ylr, y_predict_training))\n",
    "\n",
    "print(\"[Test Classification Report:]\")\n",
    "print(classification_report(ytestlr, y_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
